{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rock, Paper, Scissor Hand Detection using CNN\n",
    "\n",
    "**This project is a work in progress as a requirement for Dicoding Machine Learning Path**\n",
    "\n",
    "\n",
    "- Name : Andi Dwi Prastyo\n",
    "- College : state polytechnic of malang\n",
    "- Major : Information Technology\n",
    "- Study Program : D4 - Informatics Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import splitfolders\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-01 18:46:58--  https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... failed: No route to host.\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying RPS CV Images to a Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 2188 files [00:00, 4726.26 files/s]\n"
     ]
    }
   ],
   "source": [
    "splitfolders.ratio('rockpaperscissors/rps-cv-images', output=\"rockpaperscissors/data\", seed=1, ratio=(.6, .4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Directory for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'rockpaperscissors/data'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rock = os.path.join(train_dir, 'rock')\n",
    "train_paper = os.path.join(train_dir, 'paper')\n",
    "train_scissors = os.path.join(train_dir, 'scissors')\n",
    "\n",
    "validation_rock = os.path.join(validation_dir, 'rock')\n",
    "validation_paper = os.path.join(validation_dir, 'paper')\n",
    "validation_scissors = os.path.join(validation_dir, 'scissors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, rotation_range=20, horizontal_flip=True, fill_mode='nearest')\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1312 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 876 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(224,224), batch_size=32, color_mode='rgb', class_mode='categorical', shuffle=True, seed=42) \n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir, target_size=(224,224), batch_size=32, color_mode='rgb', class_mode='categorical', shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_13 (Conv2D)          (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPooli  (None, 111, 111, 32)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 54, 54, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPooli  (None, 26, 26, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPooli  (None, 12, 12, 128)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               9437696   \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9680067 (36.93 MB)\n",
      "Trainable params: 9680067 (36.93 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "25/25 [==============================] - 45s 2s/step - loss: 1.0948 - accuracy: 0.3688 - val_loss: 1.0230 - val_accuracy: 0.4125\n",
      "Epoch 2/25\n",
      "25/25 [==============================] - 43s 2s/step - loss: 0.6541 - accuracy: 0.7500 - val_loss: 0.2992 - val_accuracy: 0.9000\n",
      "Epoch 3/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.3608 - accuracy: 0.8675 - val_loss: 0.3242 - val_accuracy: 0.8938\n",
      "Epoch 4/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.2485 - accuracy: 0.9137 - val_loss: 0.1137 - val_accuracy: 0.9750\n",
      "Epoch 5/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.2155 - accuracy: 0.9187 - val_loss: 0.1565 - val_accuracy: 0.9375\n",
      "Epoch 6/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.2543 - accuracy: 0.9137 - val_loss: 0.1807 - val_accuracy: 0.9187\n",
      "Epoch 7/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.1365 - accuracy: 0.9550 - val_loss: 0.0890 - val_accuracy: 0.9688\n",
      "Epoch 8/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.1831 - accuracy: 0.9450 - val_loss: 0.2055 - val_accuracy: 0.9375\n",
      "Epoch 9/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.1486 - accuracy: 0.9500 - val_loss: 0.1528 - val_accuracy: 0.9563\n",
      "Epoch 10/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.1558 - accuracy: 0.9538 - val_loss: 0.0704 - val_accuracy: 0.9812\n",
      "Epoch 11/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.1100 - accuracy: 0.9650 - val_loss: 0.1464 - val_accuracy: 0.9625\n",
      "Epoch 12/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.1420 - accuracy: 0.9675 - val_loss: 0.1007 - val_accuracy: 0.9500\n",
      "Epoch 13/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.1069 - accuracy: 0.9663 - val_loss: 0.0727 - val_accuracy: 0.9750\n",
      "Epoch 14/25\n",
      "25/25 [==============================] - 43s 2s/step - loss: 0.1014 - accuracy: 0.9638 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
      "Epoch 15/25\n",
      "25/25 [==============================] - 43s 2s/step - loss: 0.0901 - accuracy: 0.9712 - val_loss: 0.0635 - val_accuracy: 0.9875\n",
      "Epoch 16/25\n",
      "25/25 [==============================] - 45s 2s/step - loss: 0.0605 - accuracy: 0.9725 - val_loss: 0.0371 - val_accuracy: 0.9750\n",
      "Epoch 17/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.0898 - accuracy: 0.9688 - val_loss: 0.1313 - val_accuracy: 0.9812\n",
      "Epoch 18/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.0539 - accuracy: 0.9825 - val_loss: 0.0764 - val_accuracy: 0.9875\n",
      "Epoch 19/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.0637 - accuracy: 0.9787 - val_loss: 0.0721 - val_accuracy: 0.9750\n",
      "Epoch 20/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.1067 - accuracy: 0.9675 - val_loss: 0.0976 - val_accuracy: 0.9563\n",
      "Epoch 21/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.0561 - accuracy: 0.9787 - val_loss: 0.0515 - val_accuracy: 0.9812\n",
      "Epoch 22/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.0531 - accuracy: 0.9825 - val_loss: 0.0964 - val_accuracy: 0.9625\n",
      "Epoch 23/25\n",
      "25/25 [==============================] - 45s 2s/step - loss: 0.0654 - accuracy: 0.9800 - val_loss: 0.0419 - val_accuracy: 0.9937\n",
      "Epoch 24/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.0381 - accuracy: 0.9850 - val_loss: 0.1716 - val_accuracy: 0.9750\n",
      "Epoch 25/25\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.0330 - accuracy: 0.9925 - val_loss: 0.1232 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fa7d8087700>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(train_generator, steps_per_epoch=25, epochs=25, verbose=1, validation_data=validation_generator, validation_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
